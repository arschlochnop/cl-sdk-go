# Crawlab Go SDK

è‰¹ï¼Crawlabå®˜æ–¹Go SDKï¼Œé›¶ä¾èµ–ã€çº¯æ ‡å‡†åº“å®ç°ï¼

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
go get github.com/arschlochnop/cl-sdk-go
```

### 5è¡Œä»£ç ä¸Šæ‰‹

```go
package main

import "github.com/arschlochnop/cl-sdk-go"

func main() {
    crawlab.SaveItem(map[string]interface{}{"title": "Hello"})
}
```

## ğŸ“¦ æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | æ–‡ä»¶ | è¯´æ˜ | ä»£ç é‡ |
|-----|------|------|--------|
| åŸºç¡€IPC | sdk.go | æ ¸å¿ƒé€šä¿¡å‡½æ•° | 191è¡Œ |
| Spideræ¥å£ | spider.go | çˆ¬è™«åŸºç±»ï¼ˆå¯é€‰ï¼‰ | 180è¡Œ |
| é…ç½®ç®¡ç† | config.go | ç¯å¢ƒå˜é‡é…ç½® | 160è¡Œ |
| é‡è¯•æœºåˆ¶ | retry.go | è‡ªåŠ¨é‡è¯• | 170è¡Œ |
| HTTPå®¢æˆ·ç«¯ | httpclient.go | ç½‘ç»œè¯·æ±‚ | 181è¡Œ |

**æ€»è®¡ï¼š882è¡Œï¼Œé›¶å¤–éƒ¨ä¾èµ–ï¼**

## ğŸ“– APIå‚è€ƒ

### 1. æ•°æ®ä¿å­˜

```go
// ä¿å­˜å•æ¡æ•°æ®
func SaveItem(item interface{}) error

// ä¿å­˜å¤šæ¡æ•°æ®
func SaveItems(items ...interface{}) error

// æ‰¹é‡ä¿å­˜ï¼ˆæ¨èï¼‰
func SaveBatch(items []interface{}) error
```

### 2. æ—¥å¿—è¾“å‡º

```go
// åŸºç¡€æ—¥å¿—
func Log(format string, args ...interface{})

// åˆ†çº§æ—¥å¿—
func LogInfo(format string, args ...interface{})
func LogError(format string, args ...interface{})
func LogWarn(format string, args ...interface{})
func LogDebug(format string, args ...interface{})
```

### 3. ç¯å¢ƒå˜é‡

```go
// è·å–Crawlabç¯å¢ƒå˜é‡
func GetTaskID() string      // ä»»åŠ¡ID
func GetSpiderID() string    // çˆ¬è™«ID
func GetNodeID() string      // èŠ‚ç‚¹ID
func GetParam() string       // ä»»åŠ¡å‚æ•°
func GetScheduleID() string  // è°ƒåº¦ID

// å·¥å…·å‡½æ•°
func MustGetEnv(key string) string  // å¿…é¡»å­˜åœ¨
func GetEnv(key, defaultValue string) string
func ParseParamJSON(v interface{}) error  // è§£æJSONå‚æ•°
```

### 4. Spideræ¥å£ï¼ˆå¯é€‰ï¼‰

```go
// Spideræ¥å£
type Spider interface {
    Run(ctx context.Context) error
}

// åˆ›å»ºçˆ¬è™«
spider := crawlab.NewSpider("MySpider")

// ä¿å­˜æ•°æ®ï¼ˆè‡ªåŠ¨ç»Ÿè®¡ï¼‰
spider.Save(data)
spider.SaveBatch(items)

// è¾“å‡ºæ—¥å¿—
spider.LogInfo("æ¶ˆæ¯")
spider.LogError("é”™è¯¯")

// ç»Ÿè®¡ä¿¡æ¯
spider.Stats.ItemsSaved  // ä¿å­˜æ•°
spider.Stats.Requests    // è¯·æ±‚æ•°
spider.Stats.Errors      // é”™è¯¯æ•°

// æ‰§è¡Œçˆ¬è™«ï¼ˆè‡ªåŠ¨å¤„ç†å¼‚å¸¸ï¼‰
spider.Execute(spider)
```

### 5. é…ç½®ç®¡ç†

```go
// åŠ è½½é…ç½®
config := crawlab.LoadConfig()

// é…ç½®å­—æ®µ
config.MaxRetries      // æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰
config.RetryDelay      // é‡è¯•å»¶è¿Ÿï¼ˆé»˜è®¤2ç§’ï¼‰
config.RequestTimeout  // è¯·æ±‚è¶…æ—¶ï¼ˆé»˜è®¤30ç§’ï¼‰
config.MaxConcurrency  // æœ€å¤§å¹¶å‘ï¼ˆé»˜è®¤10ï¼‰
config.BatchSize       // æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤100ï¼‰

// ç¯å¢ƒå˜é‡è¦†ç›–
// CRAWLAB_MAX_RETRIES=5
// CRAWLAB_BATCH_SIZE=200
```

### 6. é‡è¯•æœºåˆ¶

```go
// åŸºç¡€é‡è¯•
err := crawlab.Retry(fn, maxRetries, delay)

// Contextæ”¯æŒ
err := crawlab.RetryWithContext(ctx, fn, maxRetries, delay)

// æŒ‡æ•°é€€é¿
err := crawlab.RetryWithBackoff(ctx, fn, maxRetries, initialDelay, maxDelay)

// æ¡ä»¶é‡è¯•
err := crawlab.RetryIf(ctx, fn, shouldRetry, maxRetries, delay)
```

### 7. HTTPå®¢æˆ·ç«¯

```go
// åˆ›å»ºå®¢æˆ·ç«¯
client := crawlab.NewHTTPClient(30 * time.Second)

// è®¾ç½®Header
client.SetHeader("User-Agent", "Crawlab/1.0")

// è®¾ç½®é‡è¯•
client.SetRetry(3, 2*time.Second)

// å‘é€è¯·æ±‚
resp, err := client.Get(ctx, url)
resp, err := client.Post(ctx, url, body)
resp, err := client.Put(ctx, url, body)
resp, err := client.Delete(ctx, url)
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### çº¯å‡½æ•°å¼

```go
func main() {
    // ä¿å­˜æ•°æ®
    data := map[string]interface{}{"title": "Example"}
    crawlab.SaveItem(data)

    // è¾“å‡ºæ—¥å¿—
    crawlab.LogInfo("å®Œæˆ")
}
```

### Spideræ¥å£

```go
type MySpider struct {
    *crawlab.BaseSpider
}

func (s *MySpider) Run(ctx context.Context) error {
    s.Save(map[string]interface{}{"title": "Example"})
    s.LogInfo("å®Œæˆ")
    return nil
}

func main() {
    spider := &MySpider{
        BaseSpider: crawlab.NewSpider("MySpider"),
    }
    spider.Execute(spider)
}
```

### HTTPçˆ¬è™«

```go
type WebSpider struct {
    *crawlab.BaseSpider
    client *crawlab.HTTPClient
}

func (s *WebSpider) Run(ctx context.Context) error {
    // åˆ›å»ºHTTPå®¢æˆ·ç«¯
    s.client = crawlab.NewHTTPClient(30 * time.Second)
    s.client.SetRetry(3, 2*time.Second)

    // å‘é€è¯·æ±‚
    resp, err := s.client.Get(ctx, "https://example.com")
    if err != nil {
        return err
    }
    defer resp.Body.Close()

    // è¯»å–æ•°æ®
    body, _ := io.ReadAll(resp.Body)

    // ä¿å­˜
    s.Save(map[string]interface{}{
        "url":  resp.Request.URL.String(),
        "size": len(body),
    })

    return nil
}
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–

```go
// âŒ ä½æ•ˆï¼š1000æ¬¡IPCè°ƒç”¨
for i := 0; i < 1000; i++ {
    crawlab.SaveItem(data)
}

// âœ… é«˜æ•ˆï¼š1æ¬¡IPCè°ƒç”¨
items := make([]interface{}, 1000)
crawlab.SaveBatch(items)
```

### 2. é”™è¯¯å¤„ç†

```go
// âœ… æ¨èï¼šExecuteè‡ªåŠ¨å¤„ç†å¼‚å¸¸
spider.Execute(spider)

// âŒ ä¸æ¨èï¼šæ‰‹åŠ¨å¤„ç†
if err := spider.Run(ctx); err != nil {
    panic(err)
}
```

### 3. æ•°æ®å¤§å°æ£€æŸ¥

```go
// SDKè‡ªåŠ¨æ£€æŸ¥æ•°æ®å¤§å°
// è¶…è¿‡5MBä¼šè¾“å‡ºè­¦å‘Š
crawlab.SaveItem(largeData)  // è‡ªåŠ¨è­¦å‘Š

// å»ºè®®ï¼šå¤§æ•°æ®ä½¿ç”¨å¤–éƒ¨å­˜å‚¨ï¼ˆOSSã€S3ç­‰ï¼‰
```

## ğŸ“ ç¯å¢ƒå˜é‡

### Crawlabå†…ç½®

| å˜é‡å | è¯´æ˜ |
|--------|------|
| `CRAWLAB_TASK_ID` | ä»»åŠ¡ID |
| `CRAWLAB_SPIDER_ID` | çˆ¬è™«ID |
| `CRAWLAB_NODE_ID` | èŠ‚ç‚¹ID |
| `CRAWLAB_TASK_PARAM` | ä»»åŠ¡å‚æ•°ï¼ˆJSONï¼‰ |
| `CRAWLAB_SCHEDULE_ID` | è°ƒåº¦ID |

### SDKé…ç½®

| å˜é‡å | ç±»å‹ | é»˜è®¤å€¼ |
|--------|------|--------|
| `CRAWLAB_MAX_RETRIES` | int | 3 |
| `CRAWLAB_RETRY_DELAY` | duration | 2s |
| `CRAWLAB_REQUEST_TIMEOUT` | duration | 30s |
| `CRAWLAB_MAX_CONCURRENCY` | int | 10 |
| `CRAWLAB_BATCH_SIZE` | int | 100 |

## ğŸ”— ç›¸å…³é“¾æ¥

- [ç¤ºä¾‹ä»£ç ](../examples/)
- [å¿«é€Ÿå¼€å§‹](../examples/QUICKSTART.md)
- [Crawlabæ–‡æ¡£](https://docs.crawlab.cn/)

---

è‰¹ï¼è½»é‡é«˜æ•ˆçš„SDKï¼Œè€ç‹ç²¾å¿ƒæ‰“é€ ï¼
